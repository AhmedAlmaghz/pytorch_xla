


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>وقت تشغيل PJRT &mdash; PyTorch/XLA master documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="PyTorch/XLA SPMD الدليل الإرشادي للمستخدم" href="spmd.html" />
    <link rel="prev" title="عمليات كمية لأجهزة XLA (ميزة تجريبية)" href="quantized_ops.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (2.4.0 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">وثائق PyTorch/XLA</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">الوثائق</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="debug.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="eager_mode.html">وضع Eager + واجهة برمجة تطبيقات Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="eager_mode.html#id1">الخلفية</a></li>
<li class="toctree-l1"><a class="reference internal" href="eager_mode.html#id2">الاستخدام الأساسي</a></li>
<li class="toctree-l1"><a class="reference internal" href="eager_mode.html#id5">المعيار المرجعي</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu.html">كيفية التشغيل باستخدام PyTorch/XLA:GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_process_distributed.html">كيفية تنفيذ DistributedDataParallel (DDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantized_ops.html">عمليات كمية لأجهزة XLA (ميزة تجريبية)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">وقت تشغيل PJRT</a></li>
<li class="toctree-l1"><a class="reference internal" href="#pytorch-xla-r2-1">الميزات الجديدة في PyTorch/XLA r2.1:</a></li>
<li class="toctree-l1"><a class="reference internal" href="#pytorch-xla-r2-0">الميزات الجديدة في PyTorch/XLA r2.0:</a></li>
<li class="toctree-l1"><a class="reference internal" href="#id2">خلاصة القول</a></li>
<li class="toctree-l1"><a class="reference internal" href="#id4">الفوائد</a></li>
<li class="toctree-l1"><a class="reference internal" href="#id5">البدء السريع</a></li>
<li class="toctree-l1"><a class="reference internal" href="#id11">الاختلافات عن XRT</a></li>
<li class="toctree-l1"><a class="reference internal" href="#pjrt-torch-distributed">PJRT و torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="#id14">الأداء</a></li>
<li class="toctree-l1"><a class="reference internal" href="#id15">وقت تشغيل TPU الجديد</a></li>
<li class="toctree-l1"><a class="reference internal" href="spmd.html">PyTorch/XLA SPMD الدليل الإرشادي للمستخدم</a></li>
<li class="toctree-l1"><a class="reference internal" href="spmd.html#fully-sharded-data-parallel-fsdp-spmd">Fully Sharded Data Parallel(FSDP) عبر SPMD</a></li>
<li class="toctree-l1"><a class="reference internal" href="spmd.html#id17">PyTorch/XLA SPMD المواضيع المتقدمة</a></li>
<li class="toctree-l1"><a class="reference internal" href="spmd.html#id26">الحفظ الموزع للإشارات المرجعية</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile.html">تكامل TorchDynamo (torch.compile) في PyTorch XLA</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>وقت تشغيل PJRT</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/runtime.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="pjrt">
<h1>وقت تشغيل PJRT<a class="headerlink" href="#pjrt" title="Permalink to this heading">¶</a></h1>
<p>تمت هجرة PyTorch/XLA من وقت تشغيل XRT القائم على TensorFlow إلى <a class="reference external" href="https://github.com/openxla/xla/tree/main/xla/pjrt">وقت تشغيل PJRT</a> الذي يستخدمه <a class="reference external" href="https://github.com/google/jax">JAX</a>.</p>
<p>إذا صادفتك مشكلة مع PJRT، يرجى تقديم تقرير عن المشكلة على GitHub مع علامة “وقت التشغيل”.</p>
</div>
<div class="section" id="pytorch-xla-r2-1">
<h1>الميزات الجديدة في PyTorch/XLA r2.1:<a class="headerlink" href="#pytorch-xla-r2-1" title="Permalink to this heading">¶</a></h1>
<ul class="simple">
<li><p>أصبح PJRT مستقرًا في PyTorch/XLA r2.1!</p></li>
<li><p>انتقلت واجهات برمجة التطبيقات العامة للوقت التشغيل من <code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt</span></code> إلى <code class="docutils literal notranslate"><span class="pre">torch_xla.runtime</span></code>.</p></li>
<li><p>تمت إعادة تسمية طريقة المبادرة <code class="docutils literal notranslate"><span class="pre">pjrt://</span></code> إلى <code class="docutils literal notranslate"><span class="pre">xla://</span></code>، وهي مسجلة بواسطة <code class="docutils literal notranslate"><span class="pre">torch_xla.distributed.xla_backend</span></code>.</p></li>
<li><p>لا تزال الأسماء السابقة <code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.*</span></code> متوفرة في هذا الإصدار للتوافق.</p></li>
<li><p>أصبح <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> مدعومًا الآن عند استخدام <code class="docutils literal notranslate"><span class="pre">init_method='xla://'</span></code>.</p></li>
<li><p>إضافات جديدة لXPU وNeuron عبر واجهة برمجة تطبيقات C في PJRT.</p></li>
</ul>
</div>
<div class="section" id="pytorch-xla-r2-0">
<h1>الميزات الجديدة في PyTorch/XLA r2.0:<a class="headerlink" href="#pytorch-xla-r2-0" title="Permalink to this heading">¶</a></h1>
<ul class="simple">
<li><p>سيتم تكوين PJRT بشكل افتراضي إذا لم تقم بإدخال أي تكوين وقت تشغيل آخر. إذا استمررت في تعيين تكوين XRT (<code class="docutils literal notranslate"><span class="pre">XRT_TPU_CONFIG</span></code>)، فلن يكون لهذا التغيير أي تأثير.</p></li>
<li><p>تحسين الأداء بنسبة تصل إلى 30% باستخدام تنفيذ وقت تشغيل TPU الجديد في <code class="docutils literal notranslate"><span class="pre">libtpu</span></code>.</p></li>
<li><p>تنفيذ جديد لـ <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> يمكنه التوسع إلى آلاف من أنوية TPU.</p></li>
<li><p>[تجريبي] دعم <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> لـ TPU v2 وv3، بما في ذلك <code class="docutils literal notranslate"><span class="pre">pjrt://</span></code> <code class="docutils literal notranslate"><span class="pre">init_method</span></code>.</p></li>
</ul>
</div>
<div class="section" id="id2">
<h1>خلاصة القول<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h1>
<ul class="simple">
<li><p>لاستخدام وقت تشغيل معاينة PJRT، قم بتعيين متغير البيئة <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE</span></code> إلى <code class="docutils literal notranslate"><span class="pre">CPU</span></code> أو <code class="docutils literal notranslate"><span class="pre">TPU</span></code> أو <code class="docutils literal notranslate"><span class="pre">CUDA</span></code>.</p></li>
<li><p>في XRT، تكون جميع الأحمال العملة الموزعة متعددة العمليات، مع وجود عملية واحدة لكل جهاز. على TPU v2 وv3 في PJRT، تكون الأحمال العملة متعددة العمليات ومتعددة الخيوط (4 عمليات مع خيطين لكل منها)، لذلك يجب أن يكون حمل العمل الخاص بك آمنًا للخيوط. راجع <a class="reference external" href="#multithreading-on-tpu-v2v3">تعدد الخيوط على TPU v2/v3</a> و<a class="reference external" href="https://github.com/pytorch/xla/blob/master/API_GUIDE.md#running-on-multiple-xla-devices-with-multi-processing">قسم تعدد العمليات في دليل واجهة برمجة التطبيقات</a> لمزيد من المعلومات. الاختلافات الرئيسية التي يجب مراعاتها:</p></li>
<li><p>لتهيئة نموذج بطريقة آمنة للخيوط، قم ببث المعلمات عبر النسخ المتماثلة بعد التهيئة (<code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt.broadcast_master_param</span></code>) أو قم بتحميل معلمات النسخة المتماثلة لكل منها من نقطة تفتيش مشتركة.</p></li>
<li><p>بالنسبة لتوليد الأرقام العشوائية الأخرى، استخدم <code class="docutils literal notranslate"><span class="pre">torch.Generator</span></code> حيثما أمكن ذلك. إن مولد RNG العالمي لـ <code class="docutils literal notranslate"><span class="pre">torch</span></code> ليس آمنًا للخيوط، حتى إذا قمت بتعيين نفس <code class="docutils literal notranslate"><span class="pre">torch.manual_seed</span></code> عبر النسخ المتماثلة.</p></li>
<li><p>لاستخدام <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>، قم باستيراد <code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt_backend</span></code> واستخدم <code class="docutils literal notranslate"><span class="pre">xla://</span></code> <code class="docutils literal notranslate"><span class="pre">init_method</span></code>.</p></li>
<li><p>تعد هذه الخطوات اختيارية لـ GPU وTPU v4.</p></li>
</ul>
<p>مثال على الفرق من XRT إلى PJRT:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>import os

<span class="w"> </span>import torch
<span class="w"> </span>import torch.nn as nn
<span class="w"> </span>from torch.nn.parallel import DistributedDataParallel as DDP
<span class="w"> </span>import torch.optim as optim
<span class="w"> </span>import torch.distributed as dist
<span class="w"> </span>import torch_xla
<span class="w"> </span>import torch_xla.core.xla_model as xm
<span class="w"> </span>import torch_xla.distributed.parallel_loader as pl
<span class="w"> </span>import torch_xla.distributed.xla_backend
<span class="gi">+import torch_xla.runtime as xr</span>


<span class="w"> </span>def _mp_fn(index):
<span class="w"> </span>  device = xm.xla_device()
<span class="gd">-  dist.init_process_group(&#39;xla&#39;, rank=xr.global_ordinal(), world_size=xr.world_size())</span>
<span class="gi">+  dist.init_process_group(&#39;xla&#39;, init_method=&#39;xla://&#39;)</span>

<span class="w"> </span>  torch.manual_seed(42)
<span class="w"> </span>  model = nn.Linear(128, 10).to(device)

<span class="gi">+  # Optional for TPU v4 and GPU</span>
<span class="gi">+  xm.broadcast_master_param(model)</span>
<span class="w"> </span>  model = DDP(model, gradient_as_bucket_view=True)

<span class="w"> </span>  loss_fn = nn.MSELoss()
<span class="w"> </span>  optimizer = optim.SGD(model.parameters(), lr=.001)

<span class="w"> </span>  for i in range(10):
<span class="w"> </span>    data, target = torch.randn((128, 128), device=device), torch.randn((128, 10), device=device)

<span class="w"> </span>    optimizer.zero_grad()
<span class="w"> </span>    output = model(data)
<span class="w"> </span>    loss = loss_fn(output, target)
<span class="w"> </span>    loss.backward()

<span class="w"> </span>    optimizer.step()
<span class="w"> </span>    xm.mark_step()

<span class="w"> </span>  # Print mean parameters so we can confirm they&#39;re the same across replicas
<span class="w"> </span>  print([p.mean() for p in model.parameters()])

<span class="w"> </span>if __name__ == &#39;__main__&#39;:
<span class="gd">-  os.environ[&#39;XRT_TPU_CONFIG&#39;] = &#39;localservice;0;localhost:51011&#39;</span>
<span class="gd">-  os.environ[&#39;MASTER_ADDR&#39;] = &#39;localhost&#39;</span>
<span class="gd">-  os.environ[&#39;MASTER_PORT&#39;] = &#39;12355&#39;</span>

<span class="gi">+  # Recommended: set PJRT_DEVICE to your local device type</span>
<span class="gi">+  os.environ[&#39;PJRT_DEVICE&#39;] = &#39;TPU&#39;</span>

<span class="w"> </span>  torch_xla.launch(_mp_fn)
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h1>الفوائد<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h1>
<ul class="simple">
<li><p>تكوين وقت تشغيل بسيط: ما عليك سوى تعيين <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE</span></code> إلى <code class="docutils literal notranslate"><span class="pre">TPU</span></code> أو <code class="docutils literal notranslate"><span class="pre">CPU</span></code> أو <code class="docutils literal notranslate"><span class="pre">CUDA</span></code> وبدء استخدام XLA! أو، دع PJRT يختار جهازًا تلقائيًا بناءً على بيئتك.</p></li>
<li><p>تحسين الأداء: تقليل النفقات العامة من gRPC يعني تنفيذ أسرع من النهاية إلى النهاية. في TorchBench 2.0، لاحظنا تحسنًا بنسبة تزيد عن 35% في وقت التدريب على TPU v4.</p></li>
<li><p>سهولة تنفيذ pod: ما عليك سوى نسخ رمزك إلى كل عامل TPU، وتشغيلها جميعًا في نفس الوقت باستخدام <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">compute</span> <span class="pre">tpus</span> <span class="pre">tpuvm</span> <span class="pre">ssh</span> <span class="pre">--worker=all</span></code>.</p></li>
<li><p>تحسين التوسع: يزيل <a class="reference external" href="https://github.com/pytorch/xla/pull/3920">قيود XRT على أحجام المعلمات</a> ويدعم ما يصل إلى 2048 شريحة TPU.</p></li>
</ul>
</div>
<div class="section" id="id5">
<h1>البدء السريع<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h1>
<p>للبدء في استخدام PJRT مع PyTorch/XLA، كل ما عليك فعله هو تعيين متغير البيئة <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE</span></code>. إذا كنت تعمل على TPU v2 أو v3، فاستمر في القراءة لمعرفة الاختلافات بين TPU v2 وv3 وv4.</p>
<div class="section" id="id6">
<h2>وحدة المعالجة المركزية<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h2>
<p>على أي جهاز مثبت عليه PyTorch/XLA، يمكنك تشغيل مثال MNIST الخاص بنا على وحدة المعالجة المركزية كما يلي:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">CPU</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_mnist</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span>
</pre></div>
</div>
</div>
<div class="section" id="tpu">
<h2>TPU<a class="headerlink" href="#tpu" title="Permalink to this heading">¶</a></h2>
<p>لإنشاء TPU جديد مثبت عليه PyTorch/XLA r2.0:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gcloud alpha compute tpus tpu-vm create $USER-pjrt --accelerator-type=v4-8 --version=tpu-vm-v4-pt-2.0 --zone=us-central2-b --project=$PROJECT
</pre></div>
</div>
<p>على v4-8، يمكنك تشغيل مثال ResNet50 الخاص بنا كما يلي:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="o">--</span><span class="n">depth</span><span class="o">=</span><span class="mi">1</span> <span class="o">--</span><span class="n">branch</span> <span class="n">r2</span><span class="mf">.0</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">.</span><span class="n">git</span>
<span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>بشكل افتراضي، سيستخدم PJRT جميع شرائح TPU. لاستخدام شريحة TPU واحدة فقط، قم بتكوين <code class="docutils literal notranslate"><span class="pre">TPU_PROCESS_BOUNDS</span></code> و<code class="docutils literal notranslate"><span class="pre">TPU_VISIBLE_CHIPS</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TPU_PROCESS_BOUNDS</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span> <span class="n">TPU_VISIBLE_CHIPS</span><span class="o">=</span><span class="mi">0</span> <span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<div class="section" id="id7">
<h3>مجموعات<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h3>
<p>على مجموعات TPU، استخدم <code class="docutils literal notranslate"><span class="pre">gcloud</span></code> لتشغيل أمرك على كل TPU بشكل متوازٍ:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;git clone --depth=1 --branch r1.13 https://github.com/pytorch/xla.git&quot;
gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=256 --num_epochs=1&quot;
</pre></div>
</div>
</div>
<div class="section" id="docker">
<h3>Docker<a class="headerlink" href="#docker" title="Permalink to this heading">¶</a></h3>
<p>يمكنك أيضًا استخدام Docker لتشغيل حمل العمل الخاص بك في حاوية مع تثبيت مسبق لـ PyTorch/XLA:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export DOCKER_IMAGE=gcr.io/...

# Optional: authenticate docker if your image is in a private GCP repository
gcloud compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command &quot;sudo gcloud auth configure-docker&quot;

# Run your workload
gcloud compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command &quot;sudo docker run --rm --privileged --net=host -e PJRT_DEVICE=TPU $DOCKER_IMAGE python pytorch/xla/test/test_train_mp_imagenet.py --fake_data&quot;
</pre></div>
</div>
<p>لاحظ أن <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> يتطلب الوصول المتميز إلى المضيف (<code class="docutils literal notranslate"><span class="pre">--privileged</span></code>) لتعريض جهاز TPU للحاوية. يتم دعم Docker على مجموعات TPU فقط مع شبكة المضيف <code class="docutils literal notranslate"><span class="pre">--net=host</span></code> في هذا الوقت. راجع <a class="reference external" href="https://cloud.google.com/tpu/docs/run-in-container">وثائق Cloud TPU</a> لمزيد من المعلومات.</p>
</div>
</div>
<div class="section" id="gpu">
<h2>GPU<a class="headerlink" href="#gpu" title="Permalink to this heading">¶</a></h2>
</div>
<div class="section" id="id8">
<h2>التدريب على عقدة GPU واحدة<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h2>
<p>لاستخدام وحدات معالجة الرسومات مع PJRT، ما عليك سوى تعيين <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE=CUDA</span></code> وتهيئة <code class="docutils literal notranslate"><span class="pre">GPU_NUM_DEVICES</span></code> إلى عدد الأجهزة الموجودة على المضيف. على سبيل المثال:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">CUDA</span> <span class="n">GPU_NUM_DEVICES</span><span class="o">=</span><span class="mi">4</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>يمكنك أيضًا استخدام <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> لبدء التدريب متعدد وحدات معالجة الرسومات (GPU) على عقدة واحدة. على سبيل المثال،</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>PJRT_DEVICE=CUDA torchrun --nnodes 1 --nproc-per-node ${NUM_GPU_DEVICES} xla/test/test_train_mp_imagenet.py --fake_data --pjrt_distributed --batch_size=128 --num_epochs=1
</pre></div>
</div>
<p>في المثال أعلاه، يعني <code class="docutils literal notranslate"><span class="pre">--nnodes</span></code> عدد الأجهزة (الأجهزة المادية أو الافتراضية) التي سيتم استخدامها (فهو 1 نظرًا لأننا نقوم بالتدريب على عقدة واحدة). يشير <code class="docutils literal notranslate"><span class="pre">--nproc-per-node</span></code> إلى عدد أجهزة GPU التي سيتم استخدامها.</p>
</div>
<div class="section" id="id9">
<h2>التدريب متعدد العقد على GPU<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h2>
<p><strong>ملاحظة: تعمل هذه الميزة فقط لـ CUDA 12 أو أحدث</strong>. على غرار كيفية استخدام PyTorch للتدريب متعدد العقد، يمكنك تشغيل الأمر كما هو موضح أدناه:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>PJRT_DEVICE=CUDA torchrun \
--nnodes=${NUMBER_GPU_VM} \
--node_rank=${CURRENT_NODE_RANK} \
--nproc_per_node=${NUMBER_LOCAL_GPU_DEVICES} \
--rdzv_endpoint=&lt;internal_ip_address:port&gt; multinode_training.py
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--nnodes</span></code>: عدد أجهزة GPU التي سيتم استخدامها.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--node_rank</span></code>: فهرس أجهزة GPU الحالية. يمكن أن تكون القيمة 0 أو 1 أو … أو ${NUMBER_GPU_VM}-1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--nproc_per_node</span></code>: عدد أجهزة GPU التي سيتم استخدامها على الجهاز الحالي.</p></li>
<li><p><cite>–rdzv_endpoint</cite>: نقطة النهاية لجهاز GPU مع node_rank==0، على شكل <cite>host:port`</cite>. سيكون``host<code class="docutils literal notranslate"><span class="pre">عنوان</span> <span class="pre">IP</span> <span class="pre">الداخلي.</span> <span class="pre">يمكن</span> <span class="pre">أن</span> <span class="pre">يكون</span></code>port` أي منفذ متاح على الجهاز. بالنسبة للتدريب/الاستدلال على عقدة واحدة، يمكن إغفال هذا المعلم.</p></li>
</ul>
<p>على سبيل المثال، إذا كنت تريد التدريب على جهازي GPU: machine_0 وmachine_1، على جهاز GPU الأول machine_0، قم بتشغيل</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PJRT_DEVICE=CUDA torchrun \</span>
<span class="o">--</span><span class="n">nnodes</span><span class="o">=</span><span class="mi">2</span> \
<span class="o">--</span><span class="n">node_rank</span><span class="o">=</span><span class="mi">0</span> \
<span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="mi">4</span> \
<span class="o">--</span><span class="n">rdzv_endpoint</span><span class="o">=</span><span class="s2">&quot;&lt;MACHINE_0_INTERNAL_IP_ADDRESS&gt;:12355&quot;</span> <span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span>  <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">pjrt_distributed</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>على جهاز GPU الثاني، قم بتشغيل</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PJRT_DEVICE=CUDA torchrun \</span>
<span class="o">--</span><span class="n">nnodes</span><span class="o">=</span><span class="mi">2</span> \
<span class="o">--</span><span class="n">node_rank</span><span class="o">=</span><span class="mi">1</span> \
<span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="mi">4</span> \
<span class="o">--</span><span class="n">rdzv_endpoint</span><span class="o">=</span><span class="s2">&quot;&lt;MACHINE_0_INTERNAL_IP_ADDRESS&gt;:12355&quot;</span> <span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span>  <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">pjrt_distributed</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>الفرق بين الأمرين أعلاه هو <code class="docutils literal notranslate"><span class="pre">--node_rank</span></code> وربما <code class="docutils literal notranslate"><span class="pre">--nproc_per_node</span></code> إذا كنت تريد استخدام عدد مختلف من أجهزة GPU على كل جهاز. كل ما تبقى متطابق. لمزيد من المعلومات حول <code class="docutils literal notranslate"><span class="pre">torchrun</span></code>، يرجى الرجوع إلى هذه <a class="reference external" href="https://pytorch.org/docs/stable/elastic/run.html">الصفحة</a>.</p>
</div>
</div>
<div class="section" id="id11">
<h1>الاختلافات عن XRT<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h1>
<p>على الرغم من أنه في معظم الحالات نتوقع أن يعمل PJRT وXRT بشكل متبادل إلى حد كبير من منظور المستخدم النهائي (خاصة على TPU v4)، إلا أنه توجد بعض الاختلافات الدقيقة المهمة التي يجب مراعاتها. من المهم أن نلاحظ أن XRT تم تصميمه حول بنية عقدة TPU، لذلك سيقوم دائمًا بتشغيل عميل وعملية خادم، حتى على أجهزة TPU VM. وبالتالي، فإن كل دفعة من الإدخالات بها تأخير إضافي من تسلسل البيانات وإلغاء تسلسلها لإرسالها عبر الشبكة.</p>
<p>يستخدم PJRT الجهاز المحلي مباشرة دون عملية خادم وسيطة. في التكوين الافتراضي، سيقوم PJRT بإنشاء عملية واحدة لكل شريحة TPU، أو 4 عمليات لكل مضيف TPU. راجع <a class="reference external" href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">وثائق Cloud TPU</a> لمزيد من المعلومات حول بنية TPU.</p>
<ul class="simple">
<li><p>مكاسب الأداء ممكنة للأحمال العملة المقيدة بسبب النفقات العامة من gRPC.</p></li>
<li><p>في ظل XRT، تعد عملية الخادم هي العملية الوحيدة التي تتفاعل مع أجهزة TPU، ولا تملك عمليات العميل حق الوصول المباشر إلى أجهزة TPU.
عند إنشاء ملف تعريف لجهاز TPU واحد (مثل v3-8 أو v4-8)، فستلاحظ عادةً 8 مسارات جهاز (واحد لكل نواة TPU). مع PJRT، تحتوي كل عملية على شريحة واحدة، وسيعرض ملف التعريف من تلك العملية نواتي TPU فقط.</p></li>
<li><p>لنفس السبب، لا يعمل التوصيل على مجموعات TPU مع XRT، لأن عملية الخادم تعمل بشكل مستقل عن رمز نموذج المستخدم. لا يملك PJRT هذا القيد، لذا فمن الممكن توصيل نواتي TPU لكل عملية في مجموعة TPU.</p></li>
<li><p>يدعم PJRT فقط بنية VM TPU وليس لدينا خطط لدعم بنية عقدة TPU مع PJRT.</p></li>
<li><p>تكوين وقت التشغيل أبسط بكثير مع PJRT. <code class="docutils literal notranslate"><span class="pre">xla_dist</span></code> غير مطلوب لتشغيل أحمال العمل على مجموعة TPU Pod. بدلاً من ذلك، قم بنسخ رمزك إلى كل مضيف TPU (<code class="docutils literal notranslate"><span class="pre">[gcloud</span> <span class="pre">compute</span> <span class="pre">tpus</span> <span class="pre">tpu-vm</span> <span class="pre">scp](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/scp)</span></code>) وقم بتشغيل الر
## التغييرات على xm.rendezvous
<em>جديد في PyTorch/XLA r2.0</em>
مع XRT، يقوم العامل 0 بتشغيل خدمة رئيسية للشبكة، وتتصل جميع العمليات على جميع العاملين بتلك الخدمة عبر gRPC. وفي الممارسة العملية، وجدنا أن تشغيل عملية رئيسية واحدة للشبكة غير موثوق بها في وحدات TPU ذات الآلاف من الرقائق بسبب عدد الاتصالات الواردة إلى العامل 0. يمكن لعملية عميل واحدة توقيت يمكن أن يتسبب في حدوث فشل وإجبار عبء العمل بأكمله على إعادة التشغيل.
لذلك، قمنا بإعادة تنفيذ <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> باستخدام اتصال جماعي أصلي لـ XLA، وهو أكثر استقرارًا واختبارًا بشكل جيد على وحدات TPU الكبيرة. يفرض هذا قيدين جديدين مقارنة بتنفيذ XRT:</p></li>
<li><p>نظرًا لضرورة أن تصبح الحمولة جزءًا من رسم XLA، يتم استدعاء <code class="docutils literal notranslate"><span class="pre">xm.mark_step</span></code> قبل نقل البيانات وبعدها. قد يؤدي استدعاء <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> في منتصف كود النموذج إلى فرض تجميع غير مرغوب فيه.</p></li>
<li><p>نظرًا لأن XLA لا تسمح بتشغيل العمليات الجماعية على مجموعة فرعية من العاملين، يجب على جميع العاملين المشاركة في “اللقاء”.
إذا كنت تحتاج إلى السلوك القديم لـ <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> (أي نقل البيانات دون تغيير رسم XLA و/أو مزامنة مجموعة فرعية من العاملين)، ففكر في استخدام
<cite>``torch.distributed.barrier`</cite> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier">https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier</a>&gt;`_
أو
<cite>``torch.distributed.all_gather_object`</cite> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather_object">https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather_object</a>&gt;`_
مع مجموعة عمليات <code class="docutils literal notranslate"><span class="pre">gloo</span></code>. إذا كنت تستخدم أيضًا backend <code class="docutils literal notranslate"><span class="pre">xla</span></code> <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>، فيمكنك استخدام <code class="docutils literal notranslate"><span class="pre">torch.new_group</span></code> لإنشاء مجموعة فرعية من نوع <code class="docutils literal notranslate"><span class="pre">gloo</span></code>. راجع <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#monitored-barrier">هذا المثال</a> من وثائق PyTorch. ضع في اعتبارك هذه القيود:</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> غير مدعوم بالكامل على TPU v2/v3. يتم تنفيذ مجموعة فرعية فقط من العمليات مع backend <code class="docutils literal notranslate"><span class="pre">xla</span></code>، ومن المحتمل ألا يعمل <code class="docutils literal notranslate"><span class="pre">gloo</span></code> كما هو متوقع في سياق متعدد الخيوط.</p></li>
<li><p>في تجاربنا، لا يتوسع <code class="docutils literal notranslate"><span class="pre">gloo</span></code> جيدًا إلى الآلاف من رقائق TPU، لذا من المتوقع أن يكون هذا البديل أقل موثوقية من استخدام <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> مع PJRT على نطاق واسع.</p></li>
</ul>
</div>
<div class="section" id="pjrt-torch-distributed">
<h1>PJRT و torch.distributed<a class="headerlink" href="#pjrt-torch-distributed" title="Permalink to this heading">¶</a></h1>
<p><em>جديد في PyTorch/XLA r2.0</em>
عند استخدام PJRT مع <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> و
<code class="docutils literal notranslate"><span class="pre">[torch.nn.parallel.DistributedDataParallel](https://github.com/pytorch/xla/blob/master/docs/ddp.md)</span></code>
نوصي بشدة باستخدام <code class="docutils literal notranslate"><span class="pre">xla://</span></code> <code class="docutils literal notranslate"><span class="pre">init_method</span></code> الجديد، والذي يقوم تلقائيًا باكتشاف معرفات النسخ المتماثلة وحجم العالم وعنوان IP الرئيسي عن طريق استعلام وقت التشغيل. على سبيل المثال:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">from</span> <span class="nn">torch_xla.experimental</span> <span class="kn">import</span> <span class="n">pjrt</span>

<span class="c1"># Required for `xla://` init_method and `xla` backend</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_backend</span>

<span class="k">def</span> <span class="nf">_all_gather</span><span class="p">(</span><span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
  <span class="c1"># No need to pass in `rank` or `world_size`</span>
  <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s1">&#39;xla&#39;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;xla://&#39;</span><span class="p">)</span>

  <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">index</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
  <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
  <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

  <span class="n">xm</span><span class="o">.</span><span class="n">mark_step</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="n">torch_xla</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">_all_gather</span><span class="p">)</span>
</pre></div>
</div>
<p>ملاحظة: على الرغم من أن <code class="docutils literal notranslate"><span class="pre">xla://</span></code> init_method غير مطلوب على TPU v4، إلا أنه لا يزال موصى به. إذا كنت تستخدم <code class="docutils literal notranslate"><span class="pre">env://</span></code>، فيجب تعيين <code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code> على عنوان IP المضيف الذي يحتوي على الجهاز 0، والذي ليس دائمًا العامل 0. تقوم طريقة init_method <code class="docutils literal notranslate"><span class="pre">xla://</span></code> بالعثور على هذا عنوان IP تلقائيًا.
ملاحظة: بالنسبة لـ TPU v2/v3، لا يزال يتعين عليك استيراد <code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt_backend</span></code>، حيث أن دعم TPU v2/v3 في <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> لا يزال تجريبيًا.
لمزيد من المعلومات حول استخدام <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> على PyTorch/XLA، راجع
<cite>``ddp.md`</cite> &lt;./ddp.md&gt;`_ على TPU V4. للحصول على مثال يستخدم DDP و PJRT معًا، قم بتشغيل مثال البرنامج النصي التالي <a class="reference external" href="../test/test_train_mp_imagenet.py">example script</a> على TPU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_mnist</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">ddp</span> <span class="o">--</span><span class="n">pjrt_distributed</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">num_epochs</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="section" id="id14">
<h1>الأداء<a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h1>
<p>يظهر TorchBench تحسينات في متوسط وقت التدريب عبر المهام باستخدام PJRT مقارنة بـ XRT، مع متوسط تحسن يزيد عن 35% على TPU v4-8. تختلف الفوائد اختلافًا كبيرًا حسب المهمة ونوع النموذج، حيث تتراوح من 0% إلى 175%. يوضح الرسم البياني التالي التفاصك لكل مهمة:</p>
<a class="reference external image-reference" href="_static/img/torchbench_pjrt_vs_xrt.svg"><img alt="PJRT مقابل XRT" src="_images/torchbench_pjrt_vs_xrt.svg" /></a>
</div>
<div class="section" id="id15">
<h1>وقت تشغيل TPU الجديد<a class="headerlink" href="#id15" title="Permalink to this heading">¶</a></h1>
<p><em>جديد في PyTorch/XLA r2.0</em>
يقدم إصدار PyTorch/XLA r2.0 الدعم لـ <a class="reference external" href="https://github.com/openxla/community/blob/main/rfcs/20230123-pjrt-plugin.md#rfc-openxla-pjrt-plugin">PJRT Plugin API</a>، المستخدم للوصول إلى وقت تشغيل TPU الجديد القائم على TFRT في <code class="docutils literal notranslate"><span class="pre">libtpu</span></code>. هذا هو وقت التشغيل الافتراضي الآن عند تعيين <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE=TPU</span></code>. سيظل وقت تشغيل TPU الموروث القائم على StreamExecutor المستخدم في الإصدار 1.13 متاحًا مع <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE=TPU_LEGACY</span></code> في الإصدار 2.0، ولكنه سيتم إزالته في إصدار مستقبلي. إذا واجهت مشكلة تحدث فقط على <code class="docutils literal notranslate"><span class="pre">TPU</span></code> وليس على <code class="docutils literal notranslate"><span class="pre">TPU_LEGACY</span></code>، يرجى إرسال تقرير عن المشكلة على GitHub.
في معظم الحالات، نتوقع أن يكون الأداء مشابهًا بين وقتَي التشغيل، ولكن في بعض الحالات، قد يكون وقت التشغيل الجديد أسرع بنسبة تصل إلى 30%. يوضح الرسم البياني التالي التفاصيل حسب المهمة:</p>
<a class="reference external image-reference" href="_static/img/torchbench_tfrt_vs_se.svg"><img alt="TFRT مقابل StreamExecutor" src="_images/torchbench_tfrt_vs_se.svg" /></a>
<p>ملاحظة: التحسينات الموضحة في هذا الرسم البياني مدرجة أيضًا في مقارنة PJRT مقابل XRT.</p>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="spmd.html" class="btn btn-neutral float-right" title="PyTorch/XLA SPMD الدليل الإرشادي للمستخدم" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="quantized_ops.html" class="btn btn-neutral" title="عمليات كمية لأجهزة XLA (ميزة تجريبية)" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright .

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">وقت تشغيل PJRT</a></li>
<li><a class="reference internal" href="#pytorch-xla-r2-1">الميزات الجديدة في PyTorch/XLA r2.1:</a></li>
<li><a class="reference internal" href="#pytorch-xla-r2-0">الميزات الجديدة في PyTorch/XLA r2.0:</a></li>
<li><a class="reference internal" href="#id2">خلاصة القول</a></li>
<li><a class="reference internal" href="#id4">الفوائد</a></li>
<li><a class="reference internal" href="#id5">البدء السريع</a><ul>
<li><a class="reference internal" href="#id6">وحدة المعالجة المركزية</a></li>
<li><a class="reference internal" href="#tpu">TPU</a><ul>
<li><a class="reference internal" href="#id7">مجموعات</a></li>
<li><a class="reference internal" href="#docker">Docker</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gpu">GPU</a></li>
<li><a class="reference internal" href="#id8">التدريب على عقدة GPU واحدة</a></li>
<li><a class="reference internal" href="#id9">التدريب متعدد العقد على GPU</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id11">الاختلافات عن XRT</a></li>
<li><a class="reference internal" href="#pjrt-torch-distributed">PJRT و torch.distributed</a></li>
<li><a class="reference internal" href="#id14">الأداء</a></li>
<li><a class="reference internal" href="#id15">وقت تشغيل TPU الجديد</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>